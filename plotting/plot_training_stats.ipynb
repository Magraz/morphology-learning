{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2693bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "config_path = Path(\"./config.yaml\")\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "base_path = Path(config[\"base_path\"])\n",
    "runs = []\n",
    "missing_paths = []\n",
    "\n",
    "for batch in config[\"batches\"]:\n",
    "    for experiment in config[\"experiments\"]:\n",
    "        for trial in config[\"trials\"]:\n",
    "            stats_path = (\n",
    "                base_path\n",
    "                / batch\n",
    "                / experiment\n",
    "                / trial\n",
    "                / \"logs\"\n",
    "                / \"training_stats_checkpoint.pkl\"\n",
    "            )\n",
    "\n",
    "            if not stats_path.exists():\n",
    "                missing_paths.append(str(stats_path))\n",
    "                continue\n",
    "\n",
    "            with stats_path.open(\"rb\") as f:\n",
    "                training_stats = pickle.load(f)\n",
    "\n",
    "            run_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"total_steps\": training_stats[\"total_steps\"],\n",
    "                    \"reward\": training_stats[\"reward\"],\n",
    "                }\n",
    "            )\n",
    "            run_df[\"batch\"] = batch\n",
    "            run_df[\"experiment\"] = experiment\n",
    "            run_df[\"trial\"] = trial\n",
    "            runs.append(run_df)\n",
    "\n",
    "if not runs:\n",
    "    raise FileNotFoundError(\"No training stats files found for the configured runs\")\n",
    "\n",
    "all_rewards_df = pd.concat(runs, ignore_index=True)\n",
    "reward_summary = (\n",
    "    all_rewards_df.groupby(\"total_steps\", as_index=False)\n",
    "    .agg(\n",
    "        mean_reward=(\"reward\", \"mean\"),\n",
    "        sem_reward=(\n",
    "            \"reward\",\n",
    "            lambda s: s.std(ddof=1) / (len(s) ** 0.5) if len(s) > 1 else 0.0,\n",
    "        ),\n",
    "        n_runs=(\"reward\", \"size\"),\n",
    "    )\n",
    "    .sort_values(\"total_steps\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Loaded {len(runs)} run(s) across {all_rewards_df['batch'].nunique()} batch(es).\"\n",
    ")\n",
    "if missing_paths:\n",
    "    print(f\"Skipped {len(missing_paths)} missing run file(s).\")\n",
    "\n",
    "reward_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b41051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = reward_summary[\"total_steps\"]\n",
    "y = reward_summary[\"mean_reward\"]\n",
    "sem = reward_summary[\"sem_reward\"]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, y, label=\"Mean reward\")\n",
    "plt.fill_between(x, y - sem, y + sem, alpha=0.2, label=\"SEM\")\n",
    "plt.xlabel(\"Total environment steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward Across Batches (mean Â± SEM)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
