{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d84073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dhg import Graph, Hypergraph\n",
    "from dhg.data import Cora\n",
    "from dhg.models import HGNN\n",
    "from dhg.random import set_seed\n",
    "from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator\n",
    "\n",
    "set_seed(2022)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rldac3qekid",
   "metadata": {},
   "source": [
    "## Load Cora Dataset and Build Hypergraph\n",
    "\n",
    "Cora is a citation network with 2,708 papers (nodes) across 7 classes. Each paper has a 1,433-dim feature vector (bag-of-words).\n",
    "\n",
    "We convert the citation graph into a hypergraph using **k-hop neighborhoods**: each vertex's 1-hop neighborhood becomes a hyperedge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i38wu5nysxh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cora dataset (auto-downloads on first run)\n",
    "data = Cora()\n",
    "\n",
    "X, lbl = data[\"features\"], data[\"labels\"]\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "# Build a graph from the edge list, then convert to hypergraph via 1-hop neighborhoods\n",
    "G = Graph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "HG = Hypergraph.from_graph_kHop(G, k=1)\n",
    "\n",
    "print(f\"Vertices: {data['num_vertices']}, Features: {data['dim_features']}, Classes: {data['num_classes']}\")\n",
    "print(f\"Train: {train_mask.sum()}, Val: {val_mask.sum()}, Test: {test_mask.sum()}\")\n",
    "print(f\"Hyperedges: {HG.num_e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u0tesb6az99",
   "metadata": {},
   "source": [
    "## Define HGNN Model and Training\n",
    "\n",
    "HGNN is a 2-layer hypergraph neural network using `HGNNConv` layers with a hidden dim of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mh55o7p7gtj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and evaluator\n",
    "net = HGNN(data[\"dim_features\"], 16, data[\"num_classes\"], drop_rate=0.5)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "# Move everything to device\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "HG = HG.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s0aauea51fh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 500\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, HG)\n",
    "    loss = F.cross_entropy(outs[train_mask], lbl[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # Validate\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        outs = net(X, HG)\n",
    "        val_res = evaluator.validate(lbl[val_mask], outs[val_mask])\n",
    "\n",
    "    if val_res > best_val:\n",
    "        best_epoch = epoch\n",
    "        best_val = val_res\n",
    "        best_state = deepcopy(net.state_dict())\n",
    "\n",
    "    if epoch % 50 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | Val: {val_res:.4f}\")\n",
    "\n",
    "print(f\"\\nBest validation: {best_val:.4f} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0jordnpr9o3",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmt882m6yxe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set\n",
    "net.load_state_dict(best_state)\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    outs = net(X, HG)\n",
    "    test_res = evaluator.test(lbl[test_mask], outs[test_mask])\n",
    "\n",
    "print(f\"Test Results (best model from epoch {best_epoch}):\")\n",
    "print(test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stw20hhod5h",
   "metadata": {},
   "source": [
    "## Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zjbexxwg7om",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(train_losses)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Cross-Entropy Loss\")\n",
    "ax.set_title(\"HGNN Training Loss on Cora\")\n",
    "ax.axvline(best_epoch, color=\"r\", linestyle=\"--\", alpha=0.7, label=f\"Best val epoch ({best_epoch})\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
